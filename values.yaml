nameOverride: ""
namespaceOverride: ""
ollama:
  # -- Automatically install Ollama Helm chart from https://otwld.github.io/ollama-helm/. Use [Helm Values](https://github.com/otwld/ollama-helm/#helm-values) to configure
  enabled: true
  # -- If enabling embedded Ollama, update fullnameOverride to your desired Ollama name value, or else it will use the default ollama.name value from the Ollama chart
  fullnameOverride: "open-webui-ollama"
  # -- Example Ollama configuration with nvidia GPU enabled, automatically downloading a model, and deploying a PVC for model persistence
  ollama:
    gpu:
      enabled: true
      type: 'nvidia'
      number: 3
    models:
      pull:
        - llama3
      run:
        - llama3
  runtimeClassName: nvidia
  persistentVolume:
    enabled: true
    size: 400Gi

persistence:
  enabled: true
  size: 20Gi

pipelines:
  # -- Automatically install Pipelines chart to extend Open WebUI functionality using Pipelines: https://github.com/open-webui/pipelines
  enabled: true
  persistence:
    enabled: true
    size: 20Gi
  # -- This section can be used to pass required environment variables to your pipelines (e.g. Langfuse hostname)
  extraEnvVars: []

tika:
  # -- Automatically install Apache Tika to extend Open WebUI
  enabled: true

